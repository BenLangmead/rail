Some design notes for Tornado

Tornado lives on GitHub: https://github.com/BenLangmead/tornado

1. Package layout
 
 The source, which is primarily Python with some R, is all in the 'src'
 directory
 
 The scripts implementing the stages of the pipeline are in 'src/rnawesome'
 (should probably rename that!)  The scripts in other subdirectories of 'src'
 e.g. bowtie, interval, read, etc, are supporting scripts, with helpful
 functions and classes.
 
 Scripts for generating useful simulated datasets are in 'datasets'.  Though,
 as of 2/22/2013, there are just some dummy Flux Simulator configuration files
 in there.

 The 'example/simple' directory contains a tiny example of a dataset (in the
 *.tab files), manifest file, and script that drives the whole pipeline
 (run.sh).

 The 'tools' directory contains Makefiles for downloading and building some
 potentially useful tools and libraries.

2. Pipeline

 The Tornado pipeline emulates the DER finder pipeline, starting with sequence
 reads.  The pipeline is laid out with a MapReduce implementation in mind.  The
 script implementing the stages of the pipeline, in order, are:
 
   1) align.py
   2) splice.py
   3) merge.py
   4) walk_prenorm.py
   5) normalize.py
   6) normalize_post.py
   7) walk_fit.py
   8) ebayes.py
   9) hmm_params.py
  10) hmm.py
 
 a. Preprocess
 
  Like Myrna, the pipeline begins with a preprocessing step.   A manifest file
  containing URLs and labels is supplied.  The URLs could point to FASTQ files,
  or files of a few other different formats.  The labels indicate the
  experimental design, e.g. cancer-1, cancer-2, cancer-3, normal-1, ....  The
  preprocessing step downloads all the reads, parses them, and emits them in a
  special tab-delimited format understood by the rest of Tornado.
 
 b. Readletize & align
  
  See comments at the top of align.py
 
 c. Merge readlet intervals into per-read minimal interval sets
 
  See comments at the top of splice.py

 d. Merge readlet intervals into per-read minimal interval sets
 
  See comments at the top of merge.py
 
 e. Compile per-position, per-sample counts in preparation for normalization
 
  See comments at the top of walk_prenorm.py
 
 f. Calculate per-sample normalization factors
 
  See comments at the top of normalize.py
 
 g. Write normalization factors to file
 
  See comments at the top of normalize_post.py
 
 h. Compile per-position count vector and fit linear model to each vector
 
  See comments at the top of walk_fit.py
 
 i. Aggregate t-statistics and calculate moderated t-statistics
 
  See comments at the top of ebayes.py
 
 j. Calculate input parameters for HMM
 
  See comments at the top of hmm_params.py
 
 k. Run HMM for each dataset (both original version and all the permuted
    versions) on each genome partition
 
  See comments at the top of hmm.py

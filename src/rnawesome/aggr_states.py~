'''
aggr_states.py
(after hmm.py)

Collects the list of states from hmm and bins them into separate files according to permutation and sample

Tab-delimited output tuple columns:
 1. Sample label
 2. Dataset id (0 for test, >0 for permutations)
 3. Reference ID
 4. Reference offset (0.based)
 5. Length of run of positions with this state
 6. HMM state

Other files:
 Sample files specified by sample name and permutation number
 1. Reference ID
 2. Reference start position
 3. Reference end position
 4. HMM state

'''
import os
import sys
import argparse
import time
import pipes
import subprocess, shlex
timeSt = time.clock()

parser = argparse.ArgumentParser(description=\
    'Takes per-position counts for given samples and calculates a summary '
    'statistic such as median or 75% percentile.')
parser.add_argument(\
    '--out_dir', type=str, required=False, default="",
    help='The directory where all of the coverage vectors for each sample will be stored')
parser.add_argument(\
    '--hadoop_exe', type=str, required=False, default="",
    help='The location of the hadoop executable.')

args = parser.parse_args()

if args.hadoop_exe!="":
    fname = "%s/temp_file"%(args.out_dir)  #temp file used to make a global file handle
    print >>sys.stderr,fname
    proc = subprocess.Popen([args.hadoop_exe, 'fs', '-put', '-', fname ], stdin=subprocess.PIPE)
    proc.stdin.close()
    proc.wait()
else:
    fname = "%s/temp_file"%(args.out_dir)  #temp file used to make a global file handle
    print >>sys.stderr,fname
    samp_out = open(fname,'w')


for ln in sys.stdin:
    ln = ln.rstrip()
    toks = ln.split('\t')
    assert len(toks)==5
    samp, data_id, ref_id, ref_off, ref_len, hmm_st, = toks[0],toks[1],toks[2],int(toks[3]),int(toks[4]),int(toks[5])
    if args.hadoop_exe!="":
        proc.stdin.close()
        proc.wait()
        line = ""%()
